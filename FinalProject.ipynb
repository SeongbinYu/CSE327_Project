{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ss5RDiAimcT",
        "outputId": "1f187184-e2cc-41dc-be41-c790b6674945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "WajcSuadmeRT",
        "outputId": "a55c5701-2ac2-41a7-daae-9b87c74c75af"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import random_split, DataLoader, Subset\n",
        "import torchvision.models as models\n",
        "from tqdm import tqdm\n",
        "import timm\n",
        "import random\n",
        "from torch.amp import autocast, GradScaler\n",
        "\n",
        "\n",
        "\n",
        "# Download dataset\n",
        "path = kagglehub.dataset_download(\"steubk/wikiart\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device:\", device)\n",
        "\n",
        "# Dataset path\n",
        "dataset_path = \"/kaggle/input/wikiart\" #\n",
        "\n",
        "\n",
        "# Image transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(518),\n",
        "    #transforms.Resize(256), # for Resnet\n",
        "    transforms.CenterCrop(518),\n",
        "    #transforms.CenterCrop(224), # for Resnet\n",
        "    #transforms.RandomHorizontalFlip(), # overfitting (not used in final experiement)\n",
        "    #transforms.RandomRotation(15), # overfitting (not used in final experiement)\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 64\n",
        "epochs = 3\n",
        "learning_rate = 0.001\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "full_dataset = datasets.ImageFolder(root=dataset_path, transform=transform)\n",
        "\n",
        "# Subset_size = number of images to train and evaluate. Used for short time partial training of the dataset\n",
        "subset_size = 80020\n",
        "small_dataset = Subset(full_dataset, torch.randperm(len(full_dataset))[:subset_size])\n",
        "\n",
        "# Split into 80% training, 20% testing\n",
        "train_size = int(0.8 * subset_size) # 80% of images for training\n",
        "test_size = subset_size - train_size # 20% of images for testing\n",
        "train_dataset, test_dataset = random_split(small_dataset, [train_size, test_size]) # randomly distribute images for training and testing\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "# Load DINOv2 ViT-Large\n",
        "#backbone =models.resnet18(weights=models.ResNet18_Weights.DEFAULT) # Load Resnet for comparison\n",
        "backbone = timm.create_model('vit_large_patch14_dinov2.lvd142m', pretrained=True)\n",
        "\n",
        "# backbone (time saving)\n",
        "for param in backbone.parameters():\n",
        "    #param.requires_grad = True # unfrozen backbone\n",
        "     param.requires_grad = False # fronzen backbone\n",
        "\n",
        "# classification head\n",
        "model = nn.Sequential(\n",
        "    backbone,\n",
        "    nn.Dropout(0.5), #dropout added for overfitting issue\n",
        "    nn.Linear(1024, len(full_dataset.classes))\n",
        "    #nn.Linear(1000, len(full_dataset.classes)) # for ResNet\n",
        ").to(device)\n",
        "\n",
        "# mixed precision\n",
        "scaler = GradScaler()\n",
        "\n",
        "\n",
        "# Training setup\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train and Evaluate\n",
        "print(\"Starting training...\")\n",
        "results = []\n",
        "train_losses = []\n",
        "test_accuracies = []\n",
        "\n",
        "\n",
        "    # training\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    loop = tqdm(train_loader, desc=f\" Epoch {epoch+1}/{epochs}\", leave= False)\n",
        "    for images, labels in loop:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        with autocast(device_type= 'cuda', dtype= torch.float16):  # enable mixed precision\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()  # gradient scaling process\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        loop.set_postfix(loss=loss.item()) # view running loss in the progress bar\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(avg_loss)\n",
        "\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = correct * 100 / total\n",
        "    test_accuracies.append(accuracy)\n",
        "    print(f\" Epoch {epoch+1}: Loss={avg_loss:.4f}, Accuracy={accuracy:.2f}%\")\n",
        "\n",
        "results.append((train_losses, test_accuracies)) # could be used for result interpretation"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
